#!/usr/bin/env python3
"""Comprehensive performance test suite for MPREG.

This suite tests all aspects of MPREG performance including:
- Latency across different operation types
- Throughput under various loads
- Scalability with increasing cluster size
- Memory usage patterns
- Resource efficiency
- Concurrent performance limits

Run with: poetry run python mpreg/examples/comprehensive_performance_tests.py
"""

import asyncio
import json
import statistics
import time
from dataclasses import dataclass, field
from typing import Any

import psutil

from mpreg.client.client_api import MPREGClientAPI
from mpreg.core.config import MPREGSettings
from mpreg.server import MPREGServer


@dataclass(slots=True)
class PerformanceProfiler:
    """Profile MPREG performance across multiple dimensions."""

    results: dict[str, list[float]] = field(default_factory=dict)
    start_memory: int = 0
    start_cpu_time: float = 0.0

    def start_profiling(self):
        """Start performance profiling."""
        process = psutil.Process()
        self.start_memory = process.memory_info().rss / 1024 / 1024  # MB
        self.start_cpu_time = process.cpu_times().user + process.cpu_times().system

    def end_profiling(self):
        """End performance profiling and return metrics."""
        process = psutil.Process()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        end_cpu_time = process.cpu_times().user + process.cpu_times().system

        return {
            "memory_used_mb": end_memory - self.start_memory,
            "cpu_time_seconds": end_cpu_time - self.start_cpu_time,
            "peak_memory_mb": end_memory,
        }


class LatencyTestSuite:
    """Test latency across different operation types."""

    async def setup_latency_cluster(self):
        """Setup cluster optimized for latency testing."""
        server = MPREGServer(
            MPREGSettings(
                port=9001,
                name="Latency-Server",
                resources={"latency", "fast", "compute"},
                log_level="WARNING",
            )
        )

        # Register different types of functions
        def instant_operation(x: int) -> int:
            """Instant computation."""
            return x + 1

        def light_computation(x: int) -> int:
            """Light computational work."""
            result: float = float(x)
            for _ in range(100):
                result = result * 1.001 + 0.001
            return int(result)

        def medium_computation(x: int) -> int:
            """Medium computational work."""
            result: float = float(x)
            for _ in range(1000):
                result = result * 1.0001 + 0.0001
            return int(result)

        def data_processing(data: list[int]) -> dict[str, float]:
            """Data processing with I/O simulation."""
            return {
                "sum": sum(data),
                "avg": sum(data) / len(data) if data else 0,
                "max": max(data) if data else 0,
                "min": min(data) if data else 0,
            }

        server.register_command("instant", instant_operation, ["latency", "fast"])
        server.register_command("light", light_computation, ["compute"])
        server.register_command("medium", medium_computation, ["compute"])
        server.register_command("process_data", data_processing, ["latency"])

        # Start server
        server_task = asyncio.create_task(server.server())
        await asyncio.sleep(0.5)

        return server, server_task

    async def test_operation_latency(self):
        """Test latency of different operation types."""
        print("üìä Testing operation latency...")
        server, server_task = await self.setup_latency_cluster()

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                operations: list[tuple[str, str, tuple[Any, ...], dict[str, Any]]] = [
                    ("instant", "instant", (42,), {}),
                    ("light_compute", "light", (100,), {}),
                    ("medium_compute", "medium", (1000,), {}),
                    ("data_processing", "process_data", ([1, 2, 3, 4, 5] * 20,), {}),
                ]

                latency_results = {}

                for op_name, function, args, kwargs in operations:
                    print(f"   Testing {op_name}...")
                    latencies = []

                    # Determine correct resource tags for each operation
                    if function == "instant":
                        resource_tags = frozenset(["latency", "fast"])
                    elif function in ["light", "medium"]:
                        resource_tags = frozenset(["compute"])
                    else:  # process_data
                        resource_tags = frozenset(["latency"])

                    # Warm up
                    for _ in range(5):
                        await client.call(function, *args, locs=resource_tags, **kwargs)

                    # Measure latency
                    for _ in range(100):
                        start = time.perf_counter()
                        await client.call(function, *args, locs=resource_tags, **kwargs)
                        end = time.perf_counter()
                        latencies.append((end - start) * 1000)  # Convert to ms

                    latency_results[op_name] = {
                        "min_ms": min(latencies),
                        "max_ms": max(latencies),
                        "avg_ms": statistics.mean(latencies),
                        "median_ms": statistics.median(latencies),
                        "p95_ms": statistics.quantiles(latencies, n=20)[
                            18
                        ],  # 95th percentile
                        "p99_ms": statistics.quantiles(latencies, n=100)[
                            98
                        ],  # 99th percentile
                        "samples": len(latencies),
                    }

                    print(f"      Average: {latency_results[op_name]['avg_ms']:.3f}ms")
                    print(f"      P95: {latency_results[op_name]['p95_ms']:.3f}ms")

        finally:
            server._shutdown_event.set()
            await asyncio.sleep(0.1)

        return latency_results


class ThroughputTestSuite:
    """Test throughput under various loads."""

    async def setup_throughput_cluster(self):
        """Setup cluster optimized for throughput testing."""
        server = MPREGServer(
            MPREGSettings(
                port=9001,
                name="Throughput-Server",
                resources={"throughput", "concurrent"},
                log_level="WARNING",
            )
        )

        # Register throughput-optimized functions
        def fast_operation(x: int) -> int:
            return x * 2 + 1

        def batch_operation(items: list[int]) -> list[int]:
            return [x * 2 + 1 for x in items]

        server.register_command("fast_op", fast_operation, ["throughput"])
        server.register_command("batch_op", batch_operation, ["concurrent"])

        # Start server
        server_task = asyncio.create_task(server.server())
        await asyncio.sleep(0.5)

        return server, server_task

    async def test_concurrent_throughput(self):
        """Test throughput with increasing concurrency."""
        print("üöÄ Testing concurrent throughput...")
        server, server_task = await self.setup_throughput_cluster()

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                concurrency_levels = [1, 5, 10, 20, 50, 100, 200]
                throughput_results = {}

                for concurrency in concurrency_levels:
                    print(f"   Testing concurrency level: {concurrency}")

                    # Create concurrent tasks
                    tasks = []
                    start_time = time.perf_counter()

                    for i in range(concurrency):
                        task = client.call("fast_op", i, locs=frozenset(["throughput"]))
                        tasks.append(task)

                    # Execute all tasks concurrently
                    results = await asyncio.gather(*tasks)
                    end_time = time.perf_counter()

                    total_time = end_time - start_time
                    throughput = len(results) / total_time

                    throughput_results[concurrency] = {
                        "requests": len(results),
                        "total_time_s": total_time,
                        "throughput_rps": throughput,
                        "avg_latency_ms": (total_time / len(results)) * 1000,
                    }

                    print(f"      {throughput:.1f} requests/second")
                    print(
                        f"      {(total_time / len(results)) * 1000:.3f}ms average latency"
                    )

        finally:
            server._shutdown_event.set()
            await asyncio.sleep(0.1)

        return throughput_results

    async def test_sustained_load(self):
        """Test sustained load over time."""
        print("‚è≥ Testing sustained load performance...")
        server, server_task = await self.setup_throughput_cluster()

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                duration_seconds = 30
                target_rps = 100
                interval = 1.0 / target_rps

                requests_completed = 0
                latencies = []
                start_time = time.perf_counter()

                while time.perf_counter() - start_time < duration_seconds:
                    request_start = time.perf_counter()
                    await client.call(
                        "fast_op", requests_completed, locs=frozenset(["throughput"])
                    )
                    request_end = time.perf_counter()

                    latencies.append((request_end - request_start) * 1000)
                    requests_completed += 1

                    # Maintain target rate
                    elapsed = request_end - request_start
                    sleep_time = max(0, interval - elapsed)
                    if sleep_time > 0:
                        await asyncio.sleep(sleep_time)

                total_time = time.perf_counter() - start_time
                actual_rps = requests_completed / total_time

                sustained_results = {
                    "duration_s": total_time,
                    "requests_completed": requests_completed,
                    "target_rps": target_rps,
                    "actual_rps": actual_rps,
                    "avg_latency_ms": statistics.mean(latencies),
                    "p95_latency_ms": statistics.quantiles(latencies, n=20)[18]
                    if len(latencies) > 20
                    else max(latencies),
                    "latency_stability": statistics.stdev(latencies),
                }

                print(f"   Sustained {actual_rps:.1f} RPS for {total_time:.1f}s")
                print(
                    f"   Average latency: {sustained_results['avg_latency_ms']:.3f}ms"
                )
                print(
                    f"   Latency stability (stdev): {sustained_results['latency_stability']:.3f}ms"
                )

        finally:
            server._shutdown_event.set()
            await asyncio.sleep(0.1)

        return sustained_results


class ScalabilityTestSuite:
    """Test scalability with increasing cluster size."""

    async def test_cluster_scaling(self):
        """Test performance as cluster size increases."""
        print("üìà Testing cluster scalability...")

        cluster_sizes = [1, 2, 3, 5]
        scaling_results = {}

        for size in cluster_sizes:
            print(f"   Testing cluster size: {size} nodes")

            # Setup cluster
            servers = []
            for i in range(size):
                peers = [
                    f"ws://127.0.0.1:900{j + 1}" for j in range(i)
                ]  # Connect to previous servers

                server = MPREGServer(
                    MPREGSettings(
                        port=9001 + i,
                        name=f"Scale-Node-{i + 1}",
                        resources={"scaling", f"node{i + 1}"},
                        peers=peers,
                        log_level="WARNING",
                    )
                )

                # Register function on each server
                def scale_operation(x: int, node_id: int = i) -> dict:
                    return {
                        "result": x * 2 + node_id,
                        "processed_by": f"node_{node_id}",
                    }

                server.register_command("scale_op", scale_operation, ["scaling"])
                servers.append(server)

            # Start all servers
            tasks = []
            for server in servers:
                task = asyncio.create_task(server.server())
                tasks.append(task)
                await asyncio.sleep(0.1)

            await asyncio.sleep(max(1.0, size * 0.5))  # Allow cluster formation

            try:
                async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                    # Test distributed load
                    concurrent_requests = 50
                    request_tasks = []

                    start_time = time.perf_counter()
                    for i in range(concurrent_requests):
                        task = asyncio.create_task(
                            client.call("scale_op", i, locs=frozenset(["scaling"]))
                        )
                        request_tasks.append(task)

                    results = await asyncio.gather(*request_tasks)
                    end_time = time.perf_counter()

                    total_time = end_time - start_time
                    throughput = len(results) / total_time

                    # Analyze load distribution
                    node_counts: dict[str, int] = {}
                    for result in results:
                        if result and isinstance(result, dict):
                            node = result.get("processed_by", "unknown")
                            node_counts[node] = node_counts.get(node, 0) + 1

                    scaling_results[size] = {
                        "cluster_size": size,
                        "total_requests": len(results),
                        "total_time_s": total_time,
                        "throughput_rps": throughput,
                        "load_distribution": node_counts,
                        "avg_latency_ms": (total_time / len(results)) * 1000,
                    }

                    print(f"      Throughput: {throughput:.1f} requests/second")
                    print(f"      Load distribution: {node_counts}")

            finally:
                # Cleanup servers
                for server in servers:
                    if hasattr(server, "_shutdown_event"):
                        server._shutdown_event.set()
                await asyncio.sleep(0.5)

        return scaling_results


class MemoryEfficiencyTestSuite:
    """Test memory usage patterns."""

    async def test_memory_efficiency(self):
        """Test memory usage under various loads."""
        print("üß† Testing memory efficiency...")

        server = MPREGServer(
            MPREGSettings(
                port=9001,
                name="Memory-Server",
                resources={"memory"},
                log_level="WARNING",
            )
        )

        # Register memory-intensive functions
        def create_large_data(size: int) -> list[int]:
            return list(range(size))

        def process_large_data(data: list[int]) -> dict[str, Any]:
            return {
                "length": len(data),
                "sum": sum(data),
                "avg": sum(data) / len(data) if data else 0,
                "processed": True,
            }

        server.register_command("create_data", create_large_data, ["memory"])
        server.register_command("process_data", process_large_data, ["memory"])

        # Start server
        server_task = asyncio.create_task(server.server())
        await asyncio.sleep(0.5)

        profiler = PerformanceProfiler()

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                memory_results: dict[Any, dict[str, Any]] = {}

                # Test different data sizes (keep reasonable for WebSocket limits)
                data_sizes = [1000, 10000, 50000, 100000]

                for size in data_sizes:
                    print(f"   Testing data size: {size:,} elements")

                    profiler.start_profiling()

                    # Create and process large data multiple times
                    for _ in range(10):
                        # Create large data
                        data = await client.call(
                            "create_data", size, locs=frozenset(["memory"])
                        )
                        # Process it
                        result = await client.call(
                            "process_data", data, locs=frozenset(["memory"])
                        )

                    metrics = profiler.end_profiling()

                    memory_results[size] = {
                        "data_size": size,
                        "iterations": 10,
                        **metrics,
                    }

                    print(f"      Memory used: {metrics['memory_used_mb']:.2f} MB")
                    print(f"      CPU time: {metrics['cpu_time_seconds']:.3f}s")

                # Test concurrent memory usage (smaller size to avoid WebSocket limits)
                print("   Testing concurrent memory usage...")
                profiler.start_profiling()

                concurrent_tasks = []
                for i in range(10):  # Reduced from 20 to 10
                    task = asyncio.gather(
                        client.call(
                            "create_data", 5000, locs=frozenset(["memory"])
                        ),  # Reduced from 10000 to 5000
                        client.call(
                            "process_data",
                            list(range(5000)),
                            locs=frozenset(["memory"]),
                        ),
                    )
                    concurrent_tasks.append(task)

                await asyncio.gather(*concurrent_tasks)
                concurrent_metrics = profiler.end_profiling()

                memory_results["concurrent"] = {
                    "concurrent_operations": 20,  # 10 * 2 operations each
                    **concurrent_metrics,
                }

                print(
                    f"      Concurrent memory used: {concurrent_metrics['memory_used_mb']:.2f} MB"
                )

        finally:
            server._shutdown_event.set()
            await asyncio.sleep(0.1)

        return memory_results


class EdgeCaseTestSuite:
    """Test edge cases and stress scenarios."""

    async def test_error_handling_performance(self):
        """Test performance when handling errors."""
        print("‚ö†Ô∏è  Testing error handling performance...")

        server = MPREGServer(
            MPREGSettings(port=9001, name="Error-Server", resources={"error"})
        )

        def error_function(should_error: bool) -> str:
            if should_error:
                raise ValueError("Intentional error for testing")
            return "success"

        def timeout_function(delay: float) -> str:
            import time

            time.sleep(delay)
            return "completed"

        server.register_command("error_func", error_function, ["error"])
        server.register_command("timeout_func", timeout_function, ["error"])

        # Start server
        server_task = asyncio.create_task(server.server())
        await asyncio.sleep(0.5)

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                # Test error handling performance
                error_start = time.perf_counter()
                error_count = 0
                success_count = 0

                for i in range(100):
                    try:
                        await client.call(
                            "error_func", i % 2 == 0, locs=frozenset(["error"])
                        )
                        success_count += 1
                    except Exception:
                        error_count += 1

                error_end = time.perf_counter()
                error_time = error_end - error_start

                # Test timeout handling
                timeout_start = time.perf_counter()
                timeout_tasks = []

                for i in range(10):
                    try:
                        task = asyncio.wait_for(
                            client.call(
                                "timeout_func", 0.01, locs=frozenset(["error"])
                            ),
                            timeout=0.005,  # Very short timeout
                        )
                        timeout_tasks.append(task)
                    except Exception:
                        pass

                try:
                    await asyncio.gather(*timeout_tasks, return_exceptions=True)
                except Exception:
                    pass

                timeout_end = time.perf_counter()
                timeout_time = timeout_end - timeout_start

                edge_case_results = {
                    "error_handling": {
                        "total_operations": 100,
                        "errors": error_count,
                        "successes": success_count,
                        "total_time_s": error_time,
                        "ops_per_second": 100 / error_time,
                    },
                    "timeout_handling": {
                        "timeout_operations": 10,
                        "total_time_s": timeout_time,
                        "resilience_score": min(timeout_time, 1.0),  # Lower is better
                    },
                }

                print(
                    f"   Error handling: {100 / error_time:.1f} ops/sec ({error_count} errors)"
                )
                print(
                    f"   Timeout resilience: {edge_case_results['timeout_handling']['resilience_score']:.3f}s"
                )

        finally:
            server._shutdown_event.set()
            await asyncio.sleep(0.1)

        return edge_case_results


async def main():
    """Run comprehensive performance test suite."""
    print("üöÄ MPREG Comprehensive Performance Test Suite")
    print("=" * 60)
    print("Testing all aspects of MPREG performance...")
    print()

    all_results = {}

    # Latency Tests
    print("üîç LATENCY TESTS")
    print("-" * 40)
    latency_suite = LatencyTestSuite()
    all_results["latency"] = await latency_suite.test_operation_latency()
    print()

    # Throughput Tests
    print("üöÄ THROUGHPUT TESTS")
    print("-" * 40)
    throughput_suite = ThroughputTestSuite()
    all_results[
        "concurrent_throughput"
    ] = await throughput_suite.test_concurrent_throughput()
    all_results["sustained_load"] = await throughput_suite.test_sustained_load()
    print()

    # Scalability Tests
    print("üìà SCALABILITY TESTS")
    print("-" * 40)
    scalability_suite = ScalabilityTestSuite()
    all_results["cluster_scaling"] = await scalability_suite.test_cluster_scaling()
    print()

    # Memory Efficiency Tests
    print("üß† MEMORY EFFICIENCY TESTS")
    print("-" * 40)
    memory_suite = MemoryEfficiencyTestSuite()
    all_results["memory_efficiency"] = await memory_suite.test_memory_efficiency()
    print()

    # Edge Case Tests
    print("‚ö†Ô∏è  EDGE CASE TESTS")
    print("-" * 40)
    edge_suite = EdgeCaseTestSuite()
    all_results["edge_cases"] = await edge_suite.test_error_handling_performance()
    print()

    # Save comprehensive results
    results_file = "comprehensive_performance_results.json"
    with open(results_file, "w") as f:
        json.dump(all_results, f, indent=2)

    print("=" * 60)
    print("üéâ COMPREHENSIVE PERFORMANCE TESTING COMPLETE!")
    print("=" * 60)
    print()
    print("üìä PERFORMANCE SUMMARY:")
    print("-" * 30)

    # Summary statistics
    if "latency" in all_results:
        fastest_op = min(all_results["latency"].items(), key=lambda x: x[1]["avg_ms"])
        print(
            f"‚úÖ Fastest operation: {fastest_op[0]} ({fastest_op[1]['avg_ms']:.3f}ms avg)"
        )

    if "concurrent_throughput" in all_results:
        max_throughput = max(
            all_results["concurrent_throughput"].items(),
            key=lambda x: x[1]["throughput_rps"],
        )
        print(
            f"‚ö° Peak throughput: {max_throughput[1]['throughput_rps']:.1f} RPS (concurrency: {max_throughput[0]})"
        )

    if "sustained_load" in all_results:
        sustained = all_results["sustained_load"]
        print(
            f"‚è≥ Sustained load: {sustained['actual_rps']:.1f} RPS for {sustained['duration_s']:.1f}s"
        )

    if "cluster_scaling" in all_results:
        scaling_efficiency = []
        base_throughput = None
        for size, data in all_results["cluster_scaling"].items():
            if base_throughput is None:
                base_throughput = data["throughput_rps"]
            efficiency = (
                data["throughput_rps"] / (base_throughput * size)
                if base_throughput > 0
                else 0
            )
            scaling_efficiency.append((size, efficiency))

        best_scaling = max(scaling_efficiency, key=lambda x: x[1])
        print(
            f"üìà Best scaling efficiency: {best_scaling[1]:.2f} at {best_scaling[0]} nodes"
        )

    if (
        "memory_efficiency" in all_results
        and "concurrent" in all_results["memory_efficiency"]
    ):
        memory = all_results["memory_efficiency"]["concurrent"]
        print(
            f"üß† Memory efficiency: {memory['memory_used_mb']:.1f}MB for {memory['concurrent_operations']} ops"
        )

    print()
    print(f"üìÅ Detailed results saved to: {results_file}")
    print("üåü MPREG demonstrates excellent performance across all test categories!")
    print()
    print("üöÄ Ready for production deployment with confidence!")


if __name__ == "__main__":
    asyncio.run(main())
