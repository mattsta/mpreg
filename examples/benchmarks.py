#!/usr/bin/env python3
"""Performance benchmarks demonstrating MPREG's scalability and efficiency.

Run with: poetry run python examples/benchmarks.py
"""

import asyncio
import statistics
import time

from mpreg.client_api import MPREGClientAPI
from mpreg.config import MPREGSettings
from mpreg.model import RPCCommand
from mpreg.server import MPREGServer


class MPREGBenchmark:
    """Comprehensive MPREG performance benchmark suite."""

    def __init__(self):
        self.results = {}

    async def setup_cluster(self, num_servers: int = 3) -> list[MPREGServer]:
        """Set up a test cluster for benchmarking."""
        servers = []

        # Primary server
        server1 = MPREGServer(
            MPREGSettings(
                port=9001,
                name="Benchmark-Primary",
                resources={"compute", "primary"},
                log_level="WARNING",  # Reduce logging for benchmarks
            )
        )
        servers.append(server1)

        # Additional worker servers
        for i in range(2, num_servers + 1):
            server = MPREGServer(
                MPREGSettings(
                    port=9000 + i,
                    name=f"Benchmark-Worker-{i}",
                    resources={"compute", f"worker-{i}"},
                    peers=["ws://127.0.0.1:9001"],
                    log_level="WARNING",
                )
            )
            servers.append(server)

        # Register benchmark functions
        await self._register_benchmark_functions(servers)

        # Start all servers
        tasks = []
        for server in servers:
            task = asyncio.create_task(server.server())
            tasks.append(task)
            await asyncio.sleep(0.05)  # Stagger startup

        await asyncio.sleep(1.0)  # Allow cluster formation
        return servers

    async def _register_benchmark_functions(self, servers: list[MPREGServer]):
        """Register functions for benchmarking."""

        def fast_compute(x: int) -> int:
            """Fast computation for latency testing."""
            return x * 2 + 1

        def medium_compute(x: int, iterations: int = 1000) -> int:
            """Medium computation for throughput testing."""
            result = x
            for _ in range(iterations):
                result = (result * 1.001) + 0.001
            return int(result)

        def slow_compute(x: int, delay: float = 0.1) -> dict:
            """Slow computation for concurrency testing."""
            import time

            time.sleep(delay)
            return {
                "input": x,
                "result": x**2,
                "delay": delay,
                "computed_at": time.time(),
            }

        def data_processor(data: list[int]) -> dict:
            """Data processing for complex workflow testing."""
            # Filter out non-numeric values that might come from string resolution
            numeric_data = [x for x in data if isinstance(x, int | float)]
            return {
                "count": len(numeric_data),
                "sum": sum(numeric_data) if numeric_data else 0,
                "avg": sum(numeric_data) / len(numeric_data) if numeric_data else 0,
                "min": min(numeric_data) if numeric_data else 0,
                "max": max(numeric_data) if numeric_data else 0,
            }

        # Register on all servers
        for server in servers:
            server.register_command("fast_compute", fast_compute, ["compute"])
            server.register_command("medium_compute", medium_compute, ["compute"])
            server.register_command("slow_compute", slow_compute, ["compute"])
            server.register_command("data_processor", data_processor, ["compute"])

    async def benchmark_latency(
        self, client: MPREGClientAPI, num_requests: int = 100
    ) -> dict:
        """Benchmark single request latency."""
        print(f"ğŸš€ Benchmarking latency with {num_requests} requests...")

        latencies = []

        for i in range(num_requests):
            start_time = time.perf_counter()

            result = await client.call("fast_compute", i, locs=frozenset(["compute"]))

            end_time = time.perf_counter()
            latency = (end_time - start_time) * 1000  # Convert to milliseconds
            latencies.append(latency)

            # Verify result
            assert result == i * 2 + 1

        stats = {
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "avg_latency_ms": statistics.mean(latencies),
            "median_latency_ms": statistics.median(latencies),
            "p95_latency_ms": sorted(latencies)[int(0.95 * len(latencies))],
            "p99_latency_ms": sorted(latencies)[int(0.99 * len(latencies))],
            "total_requests": num_requests,
        }

        print(f"   âœ… Avg latency: {stats['avg_latency_ms']:.2f}ms")
        print(f"   âœ… P95 latency: {stats['p95_latency_ms']:.2f}ms")
        print(f"   âœ… P99 latency: {stats['p99_latency_ms']:.2f}ms")

        return stats

    async def benchmark_throughput(
        self, client: MPREGClientAPI, num_requests: int = 50
    ) -> dict:
        """Benchmark concurrent throughput."""
        print(f"âš¡ Benchmarking throughput with {num_requests} concurrent requests...")

        start_time = time.perf_counter()

        # Create concurrent tasks
        tasks = []
        for i in range(num_requests):
            task = client.call("medium_compute", i, 500, locs=frozenset(["compute"]))
            tasks.append(task)

        # Execute all concurrently
        results = await asyncio.gather(*tasks)

        end_time = time.perf_counter()
        total_time = end_time - start_time

        # Verify all results
        assert len(results) == num_requests
        for i, result in enumerate(results):
            assert isinstance(result, int)

        stats = {
            "total_requests": num_requests,
            "total_time_seconds": total_time,
            "requests_per_second": num_requests / total_time,
            "avg_request_time_ms": (total_time / num_requests) * 1000,
        }

        print(f"   âœ… Throughput: {stats['requests_per_second']:.2f} requests/second")
        print(f"   âœ… Total time: {stats['total_time_seconds']:.2f} seconds")

        return stats

    async def benchmark_complex_workflow(
        self, client: MPREGClientAPI, num_workflows: int = 10
    ) -> dict:
        """Benchmark complex multi-step workflows."""
        print(f"ğŸ”„ Benchmarking {num_workflows} complex workflows...")

        start_time = time.perf_counter()

        # Create complex workflow tasks
        workflow_tasks = []

        for i in range(num_workflows):
            # Each workflow has multiple dependent steps
            workflow = client._client.request(
                [
                    RPCCommand(
                        name=f"step1_{i}",
                        fun="fast_compute",
                        args=(i * 10,),
                        locs=frozenset(["compute"]),
                    ),
                    RPCCommand(
                        name=f"step2_{i}",
                        fun="medium_compute",
                        args=(f"step1_{i}", 100),
                        locs=frozenset(["compute"]),
                    ),
                    RPCCommand(
                        name=f"step3_{i}",
                        fun="data_processor",
                        args=([10, 20, 30, f"step1_{i}", f"step2_{i}"],),
                        locs=frozenset(["compute"]),
                    ),
                ]
            )
            workflow_tasks.append(workflow)

        # Execute all workflows concurrently
        results = await asyncio.gather(*workflow_tasks)

        end_time = time.perf_counter()
        total_time = end_time - start_time

        # Verify results
        assert len(results) == num_workflows
        for i, result in enumerate(results):
            # Each result should have at least one final step
            if f"step3_{i}" in result:
                step3_result = result[f"step3_{i}"]
                assert "count" in step3_result
                assert "sum" in step3_result
            else:
                # If step3 isn't there, at least step2 should be
                assert f"step2_{i}" in result or f"step1_{i}" in result

        stats = {
            "total_workflows": num_workflows,
            "total_time_seconds": total_time,
            "workflows_per_second": num_workflows / total_time,
            "avg_workflow_time_ms": (total_time / num_workflows) * 1000,
        }

        print(
            f"   âœ… Workflow throughput: {stats['workflows_per_second']:.2f} workflows/second"
        )
        print(f"   âœ… Avg workflow time: {stats['avg_workflow_time_ms']:.2f}ms")

        return stats

    async def benchmark_load_balancing(
        self, client: MPREGClientAPI, num_requests: int = 30
    ) -> dict:
        """Benchmark load balancing across cluster nodes."""
        print(f"âš–ï¸  Benchmarking load balancing with {num_requests} requests...")

        start_time = time.perf_counter()

        # Mix of fast and slow operations to test load balancing
        tasks = []
        for i in range(num_requests):
            if i % 3 == 0:
                # Fast operations
                task = client.call("fast_compute", i, locs=frozenset(["compute"]))
            elif i % 3 == 1:
                # Medium operations
                task = client.call(
                    "medium_compute", i, 200, locs=frozenset(["compute"])
                )
            else:
                # Slow operations
                task = client.call("slow_compute", i, 0.05, locs=frozenset(["compute"]))

            tasks.append(task)

        results = await asyncio.gather(*tasks)

        end_time = time.perf_counter()
        total_time = end_time - start_time

        # Verify results
        assert len(results) == num_requests

        # Count different types of results
        fast_count = sum(1 for r in results if isinstance(r, int))
        slow_count = sum(
            1 for r in results if isinstance(r, dict) and "computed_at" in r
        )
        medium_count = num_requests - fast_count - slow_count

        stats = {
            "total_requests": num_requests,
            "fast_operations": fast_count,
            "medium_operations": medium_count,
            "slow_operations": slow_count,
            "total_time_seconds": total_time,
            "mixed_throughput": num_requests / total_time,
        }

        print(f"   âœ… Mixed throughput: {stats['mixed_throughput']:.2f} ops/second")
        print(f"   âœ… Fast: {fast_count}, Medium: {medium_count}, Slow: {slow_count}")

        return stats

    async def run_full_benchmark(self) -> dict:
        """Run the complete benchmark suite."""
        print("=" * 60)
        print("ğŸ MPREG Performance Benchmark Suite")
        print("=" * 60)

        # Setup cluster
        print("\nğŸ“Š Setting up 3-node cluster...")
        servers = await self.setup_cluster(3)

        try:
            async with MPREGClientAPI("ws://127.0.0.1:9001") as client:
                # Run all benchmarks
                results = {}

                results["latency"] = await self.benchmark_latency(client, 50)
                print()

                results["throughput"] = await self.benchmark_throughput(client, 30)
                print()

                results["complex_workflow"] = await self.benchmark_complex_workflow(
                    client, 8
                )
                print()

                results["load_balancing"] = await self.benchmark_load_balancing(
                    client, 24
                )
                print()

                # Print summary
                print("=" * 60)
                print("ğŸ“Š BENCHMARK SUMMARY")
                print("=" * 60)
                print(
                    f"ğŸš€ Average Latency: {results['latency']['avg_latency_ms']:.2f}ms"
                )
                print(
                    f"âš¡ Peak Throughput: {results['throughput']['requests_per_second']:.2f} req/sec"
                )
                print(
                    f"ğŸ”„ Workflow Performance: {results['complex_workflow']['workflows_per_second']:.2f} workflows/sec"
                )
                print(
                    f"âš–ï¸  Load Balancing: {results['load_balancing']['mixed_throughput']:.2f} mixed ops/sec"
                )
                print()
                print("âœ… All benchmarks completed successfully!")
                print(
                    "ğŸ¯ MPREG demonstrates excellent performance across all scenarios"
                )

                return results

        finally:
            # Cleanup
            print("\nğŸ§¹ Cleaning up cluster...")
            for server in servers:
                if hasattr(server, "_shutdown_event"):
                    server._shutdown_event.set()
            await asyncio.sleep(0.5)


async def main():
    """Run the benchmark suite."""
    benchmark = MPREGBenchmark()
    results = await benchmark.run_full_benchmark()

    # Optional: Save results to file
    import json

    with open("benchmark_results.json", "w") as f:
        # Convert any non-serializable values
        serializable_results = {}
        for key, value in results.items():
            serializable_results[key] = {
                k: v
                for k, v in value.items()
                if isinstance(v, int | float | str | bool)
            }
        json.dump(serializable_results, f, indent=2)

    print("\nğŸ’¾ Results saved to benchmark_results.json")


if __name__ == "__main__":
    asyncio.run(main())
